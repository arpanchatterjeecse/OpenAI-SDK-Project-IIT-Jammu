{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-03T11:35:46.688972Z",
     "iopub.status.busy": "2025-08-03T11:35:46.688626Z",
     "iopub.status.idle": "2025-08-03T11:36:02.831553Z",
     "shell.execute_reply": "2025-08-03T11:36:02.830776Z",
     "shell.execute_reply.started": "2025-08-03T11:35:46.688945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "📁 Created directories:\n",
      "   - Research notes: /kaggle/working/research_notes\n",
      "   - Temp files: /kaggle/working/temp\n",
      "🔬 Personal Research Assistant - Kaggle Edition\n",
      "============================================================\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://9245066085be5cfa23.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9245066085be5cfa23.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install arxiv requests gradio openai pathlib2 -q\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import arxiv\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Paper:\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    abstract: str\n",
    "    url: str\n",
    "    pdf_url: str = \"\"\n",
    "    venue: str = \"\"\n",
    "    year: int = 0\n",
    "    citations: int = 0\n",
    "    doi: str = \"\"\n",
    "    keywords: List[str] = None\n",
    "\n",
    "class KaggleConfig:\n",
    "    BASE_NOTES_PATH = Path(\"/kaggle/working/research_notes\")\n",
    "    TEMP_PATH = Path(\"/kaggle/working/temp\")\n",
    "    MAX_PAPERS_PER_SOURCE = 25\n",
    "    MAX_CONCURRENT_REQUESTS = 3\n",
    "    ARXIV_DELAY = 2\n",
    "    SEMANTIC_SCHOLAR_DELAY = 2\n",
    "    DEFAULT_SUMMARY_TYPE = \"comprehensive\"\n",
    "    MAX_SUMMARY_LENGTH = 400\n",
    "\n",
    "    @classmethod\n",
    "    def setup_directories(cls):\n",
    "        cls.BASE_NOTES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        cls.TEMP_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"📁 Created directories:\")\n",
    "        print(f\"   - Research notes: {cls.BASE_NOTES_PATH}\")\n",
    "        print(f\"   - Temp files: {cls.TEMP_PATH}\")\n",
    "\n",
    "# Setup directories\n",
    "KaggleConfig.setup_directories()\n",
    "\n",
    "class AcademicPaperFetcher:\n",
    "    def __init__(self):\n",
    "        self.arxiv_client = arxiv.Client()\n",
    "        self.semantic_scholar_base_url = \"https://api.semanticscholar.org/graph/v1\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def fetch_from_arxiv(self, query: str, max_results: int = 10) -> List[Paper]:\n",
    "        try:\n",
    "            print(f\"🔍 Searching ArXiv for: {query}\")\n",
    "            \n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=min(max_results, 15),\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            \n",
    "            papers = []\n",
    "            for i, result in enumerate(self.arxiv_client.results(search)):\n",
    "                if i >= max_results:\n",
    "                    break\n",
    "                    \n",
    "                paper = Paper(\n",
    "                    title=result.title.strip(),\n",
    "                    authors=[author.name for author in result.authors],\n",
    "                    abstract=result.summary.strip(),\n",
    "                    url=result.entry_id,\n",
    "                    pdf_url=result.pdf_url,\n",
    "                    year=result.published.year if result.published else 0\n",
    "                )\n",
    "                papers.append(paper)\n",
    "                \n",
    "                time.sleep(KaggleConfig.ARXIV_DELAY)\n",
    "            \n",
    "            print(f\"✅ Fetched {len(papers)} papers from ArXiv\")\n",
    "            return papers\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching from ArXiv: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def fetch_from_semantic_scholar(self, query: str, limit: int = 10) -> List[Paper]:\n",
    "        try:\n",
    "            print(f\"🔍 Searching Semantic Scholar for: {query}\")\n",
    "            \n",
    "            url = f\"{self.semantic_scholar_base_url}/paper/search\"\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"limit\": min(limit, 15),\n",
    "                \"fields\": \"title,abstract,venue,externalIds,fieldsOfStudy,year,authors,citationCount,url\"\n",
    "            }\n",
    "            \n",
    "            response = self.session.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json().get(\"data\", [])\n",
    "            \n",
    "            papers = []\n",
    "            for item in data:\n",
    "                if not item.get(\"abstract\") or len(item.get(\"abstract\", \"\")) < 50:\n",
    "                    continue\n",
    "                    \n",
    "                authors = [author.get(\"name\", \"\") for author in item.get(\"authors\", [])]\n",
    "                paper = Paper(\n",
    "                    title=item.get(\"title\", \"No title\").strip(),\n",
    "                    authors=authors,\n",
    "                    abstract=item.get(\"abstract\", \"\").strip(),\n",
    "                    url=item.get(\"url\", \"\"),\n",
    "                    venue=item.get(\"venue\", \"\"),\n",
    "                    year=item.get(\"year\", 0),\n",
    "                    citations=item.get(\"citationCount\", 0),\n",
    "                    doi=item.get(\"externalIds\", {}).get(\"DOI\", \"\"),\n",
    "                    keywords=item.get(\"fieldsOfStudy\", [])\n",
    "                )\n",
    "                papers.append(paper)\n",
    "            \n",
    "            print(f\"✅ Fetched {len(papers)} papers from Semantic Scholar\")\n",
    "            time.sleep(KaggleConfig.SEMANTIC_SCHOLAR_DELAY)\n",
    "            return papers\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching from Semantic Scholar: {e}\")\n",
    "            return []\n",
    "\n",
    "class PaperSummarizer:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key\n",
    "        self.mock_mode = not api_key\n",
    "        \n",
    "        if api_key:\n",
    "            try:\n",
    "                # Try to import openai and set up\n",
    "                import openai\n",
    "                openai.api_key = api_key\n",
    "                self.openai = openai\n",
    "            except ImportError:\n",
    "                print(\"⚠️ OpenAI package not available, using mock summaries\")\n",
    "                self.mock_mode = True\n",
    "    \n",
    "    def summarize_paper(self, paper: Paper, summary_type: str = \"comprehensive\") -> Dict[str, str]:\n",
    "        if self.mock_mode:\n",
    "            return self._generate_mock_summary(paper, summary_type)\n",
    "        \n",
    "        try:\n",
    "            prompts = {\n",
    "                \"brief\": f\"\"\"\n",
    "                Provide a brief 2-3 sentence summary of this research paper:\n",
    "                \n",
    "                Title: {paper.title}\n",
    "                Abstract: {paper.abstract[:500]}\n",
    "                \"\"\",\n",
    "                \n",
    "                \"comprehensive\": f\"\"\"\n",
    "                Provide a comprehensive summary of this research paper including:\n",
    "                1. Main research question/problem\n",
    "                2. Methodology used  \n",
    "                3. Key findings\n",
    "                4. Significance and implications\n",
    "                \n",
    "                Title: {paper.title}\n",
    "                Authors: {', '.join(paper.authors[:3])}\n",
    "                Abstract: {paper.abstract[:800]}\n",
    "                \"\"\",\n",
    "                \n",
    "                \"technical\": f\"\"\"\n",
    "                Provide a technical summary focusing on:\n",
    "                1. Technical approach and methods\n",
    "                2. Experimental setup\n",
    "                3. Results and metrics\n",
    "                4. Limitations and future work\n",
    "                \n",
    "                Title: {paper.title}\n",
    "                Abstract: {paper.abstract[:800]}\n",
    "                \"\"\"\n",
    "            }\n",
    "            \n",
    "            prompt = prompts.get(summary_type, prompts[\"comprehensive\"])\n",
    "            \n",
    "            # Updated for newer OpenAI API\n",
    "            response = self.openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert academic researcher who specializes in summarizing research papers clearly and concisely.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=KaggleConfig.MAX_SUMMARY_LENGTH,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            \n",
    "            return {\n",
    "                \"summary\": summary,\n",
    "                \"summary_type\": summary_type,\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"method\": \"openai\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ OpenAI summarization failed: {e}\")\n",
    "            return self._generate_mock_summary(paper, summary_type)\n",
    "    \n",
    "    def _generate_mock_summary(self, paper: Paper, summary_type: str) -> Dict[str, str]:\n",
    "        abstract = paper.abstract\n",
    "        \n",
    "        if summary_type == \"brief\":\n",
    "            sentences = abstract.split('. ')\n",
    "            summary = '. '.join(sentences[:2]) + '.'\n",
    "        elif summary_type == \"comprehensive\":\n",
    "            summary = f\"\"\"\n",
    "            **Research Problem:** This paper addresses {abstract.split('.')[0].lower()}.\n",
    "            \n",
    "            **Methodology:** The authors employ research methods described in their abstract to investigate the problem.\n",
    "            \n",
    "            **Key Findings:** Based on the abstract, the main contributions appear to focus on {paper.title.lower()}.\n",
    "            \n",
    "            **Significance:** This work contributes to the field by advancing understanding of the research topic.\n",
    "            \n",
    "            **Note:** This is a structured summary based on the paper's abstract. For detailed analysis, please review the full paper.\n",
    "            \"\"\"\n",
    "        else:  # technical\n",
    "            summary = f\"\"\"\n",
    "            **Technical Approach:** The methodology involves techniques related to {paper.title.lower()}.\n",
    "            \n",
    "            **Experimental Details:** Specific experimental setup details would be found in the full paper.\n",
    "            \n",
    "            **Results:** The abstract suggests findings related to the research objectives.\n",
    "            \n",
    "            **Limitations:** Full limitations would be detailed in the complete paper.\n",
    "            \n",
    "            **Note:** This is a technical overview based on available abstract information.\n",
    "            \"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"summary\": summary.strip(),\n",
    "            \"summary_type\": summary_type,\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"method\": \"mock\"\n",
    "        }\n",
    "    \n",
    "    def batch_summarize(self, papers: List[Paper], summary_type: str = \"comprehensive\") -> Dict[str, Dict]:\n",
    "        summaries = {}\n",
    "        \n",
    "        print(f\"📝 Generating {summary_type} summaries for {len(papers)} papers...\")\n",
    "        \n",
    "        max_workers = min(2, len(papers))\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_paper = {\n",
    "                executor.submit(self.summarize_paper, paper, summary_type): paper \n",
    "                for paper in papers\n",
    "            }\n",
    "            \n",
    "            for i, future in enumerate(future_to_paper):\n",
    "                paper = future_to_paper[future]\n",
    "                try:\n",
    "                    summary_data = future.result()\n",
    "                    summaries[paper.title] = summary_data\n",
    "                    print(f\"   ✅ Summarized paper {i+1}/{len(papers)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Failed to summarize: {paper.title[:50]}...\")\n",
    "                    summaries[paper.title] = {\n",
    "                        \"summary\": \"Summary generation failed\", \n",
    "                        \"error\": str(e),\n",
    "                        \"summary_type\": summary_type\n",
    "                    }\n",
    "        \n",
    "        return summaries\n",
    "\n",
    "class NotesOrganizer:\n",
    "    def __init__(self, base_path: str = None):\n",
    "        self.base_path = Path(base_path) if base_path else KaggleConfig.BASE_NOTES_PATH\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def create_folder_structure(self, topic: str) -> Path:\n",
    "        clean_topic = re.sub(r'[^\\w\\s-]', '', topic).strip()\n",
    "        clean_topic = re.sub(r'[-\\s]+', '_', clean_topic)[:50]\n",
    "        \n",
    "        topic_folder = self.base_path / clean_topic\n",
    "        topic_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        subfolders = [\"papers\", \"summaries\", \"notes\", \"references\"]\n",
    "        for subfolder in subfolders:\n",
    "            (topic_folder / subfolder).mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"📁 Created folder structure: {topic_folder}\")\n",
    "        return topic_folder\n",
    "    \n",
    "    def save_paper_data(self, paper: Paper, topic_folder: Path) -> str:\n",
    "        try:\n",
    "            papers_folder = topic_folder / \"papers\"\n",
    "            \n",
    "            clean_title = re.sub(r'[^\\w\\s-]', '', paper.title)[:50]\n",
    "            clean_title = re.sub(r'[-\\s]+', '_', clean_title)\n",
    "            \n",
    "            paper_file = papers_folder / f\"{clean_title}.json\"\n",
    "            \n",
    "            paper_data = {\n",
    "                \"title\": paper.title,\n",
    "                \"authors\": paper.authors,\n",
    "                \"abstract\": paper.abstract,\n",
    "                \"url\": paper.url,\n",
    "                \"pdf_url\": paper.pdf_url,\n",
    "                \"venue\": paper.venue,\n",
    "                \"year\": paper.year,\n",
    "                \"citations\": paper.citations,\n",
    "                \"doi\": paper.doi,\n",
    "                \"keywords\": paper.keywords,\n",
    "                \"saved_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            with open(paper_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(paper_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            return str(paper_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving paper data: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def save_summary(self, paper_title: str, summary_data: Dict, topic_folder: Path) -> str:\n",
    "        try:\n",
    "            summaries_folder = topic_folder / \"summaries\"\n",
    "            \n",
    "            clean_title = re.sub(r'[^\\w\\s-]', '', paper_title)[:50]\n",
    "            clean_title = re.sub(r'[-\\s]+', '_', clean_title)\n",
    "            \n",
    "            summary_file = summaries_folder / f\"{clean_title}_summary.txt\"\n",
    "            \n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Paper: {paper_title}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "                f.write(f\"Summary Type: {summary_data.get('summary_type', 'N/A')}\\n\")\n",
    "                f.write(f\"Generated: {summary_data.get('generated_at', 'N/A')}\\n\")\n",
    "                f.write(f\"Method: {summary_data.get('method', 'N/A')}\\n\\n\")\n",
    "                f.write(\"Summary:\\n\")\n",
    "                f.write(summary_data.get('summary', 'No summary available'))\n",
    "                f.write(\"\\n\\n\")\n",
    "            \n",
    "            return str(summary_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving summary: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def create_research_report(self, topic: str, papers: List[Paper], summaries: Dict, topic_folder: Path) -> str:\n",
    "        try:\n",
    "            report_file = topic_folder / f\"{topic.replace(' ', '_')}_research_report.md\"\n",
    "            \n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"# Research Report: {topic}\\n\\n\")\n",
    "                f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(f\"Generated in: Kaggle Notebook Environment\\n\\n\")\n",
    "                \n",
    "                f.write(f\"## Summary\\n\\n\")\n",
    "                f.write(f\"This report contains **{len(papers)} papers** related to '{topic}'.\\n\\n\")\n",
    "                \n",
    "                years = [p.year for p in papers if p.year > 0]\n",
    "                if years:\n",
    "                    f.write(f\"**Year Range:** {min(years)} - {max(years)}\\n\")\n",
    "                \n",
    "                total_citations = sum(p.citations for p in papers if p.citations > 0)\n",
    "                if total_citations > 0:\n",
    "                    f.write(f\"**Total Citations:** {total_citations}\\n\")\n",
    "                \n",
    "                f.write(\"\\n## Papers and Summaries\\n\\n\")\n",
    "                \n",
    "                for i, paper in enumerate(papers, 1):\n",
    "                    f.write(f\"### {i}. {paper.title}\\n\\n\")\n",
    "                    f.write(f\"**Authors:** {', '.join(paper.authors[:5])}\")\n",
    "                    if len(paper.authors) > 5:\n",
    "                        f.write(f\" *and {len(paper.authors)-5} others*\")\n",
    "                    f.write(\"\\n\\n\")\n",
    "                    \n",
    "                    if paper.year:\n",
    "                        f.write(f\"**Year:** {paper.year}  \")\n",
    "                    if paper.venue:\n",
    "                        f.write(f\"**Venue:** {paper.venue}  \")\n",
    "                    if paper.citations:\n",
    "                        f.write(f\"**Citations:** {paper.citations}\")\n",
    "                    f.write(\"\\n\\n\")\n",
    "                    \n",
    "                    f.write(f\"**Abstract:** {paper.abstract}\\n\\n\")\n",
    "                    \n",
    "                    summary_data = summaries.get(paper.title, {})\n",
    "                    if summary_data.get('summary'):\n",
    "                        f.write(f\"**AI Summary ({summary_data.get('summary_type', 'N/A')}):** \\n\")\n",
    "                        f.write(f\"{summary_data['summary']}\\n\\n\")\n",
    "                    \n",
    "                    f.write(f\"**URL:** [{paper.url}]({paper.url})\\n\\n\")\n",
    "                    if paper.pdf_url:\n",
    "                        f.write(f\"**PDF:** [{paper.pdf_url}]({paper.pdf_url})\\n\\n\")\n",
    "                    \n",
    "                    f.write(\"---\\n\\n\")\n",
    "                \n",
    "                f.write(\"## Generated by Personal Research Assistant\\n\")\n",
    "                f.write(\"*Powered by ArXiv and Semantic Scholar APIs*\\n\")\n",
    "            \n",
    "            print(f\"📊 Research report created: {report_file}\")\n",
    "            return str(report_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating research report: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "class PersonalResearchAssistant:\n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.fetcher = AcademicPaperFetcher()\n",
    "        self.summarizer = PaperSummarizer(openai_api_key)\n",
    "        self.organizer = NotesOrganizer()\n",
    "       \n",
    "    def research_topic(self, \n",
    "                      topic: str, \n",
    "                      max_papers: int = 10, \n",
    "                      sources: List[str] = [\"arxiv\", \"semantic_scholar\"],\n",
    "                      summary_type: str = \"comprehensive\") -> Dict[str, Any]:\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n🔬 Starting research on topic: '{topic}'\")\n",
    "            print(f\"📊 Parameters: max_papers={max_papers}, sources={sources}, summary_type={summary_type}\")\n",
    "            \n",
    "            topic_folder = self.organizer.create_folder_structure(topic)\n",
    "            \n",
    "            all_papers = []\n",
    "            \n",
    "            if \"arxiv\" in sources:\n",
    "                arxiv_papers = self.fetcher.fetch_from_arxiv(topic, max_papers // 2)\n",
    "                all_papers.extend(arxiv_papers)\n",
    "            \n",
    "            if \"semantic_scholar\" in sources:\n",
    "                scholar_papers = self.fetcher.fetch_from_semantic_scholar(topic, max_papers // 2)\n",
    "                all_papers.extend(scholar_papers)\n",
    "            \n",
    "            unique_papers = self._remove_duplicate_papers(all_papers)\n",
    "            unique_papers = unique_papers[:max_papers]\n",
    "            \n",
    "            print(f\"📚 Found {len(unique_papers)} unique papers after deduplication\")\n",
    "            \n",
    "            if not unique_papers:\n",
    "                return {\n",
    "                    \"topic\": topic,\n",
    "                    \"papers_found\": 0,\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": \"No papers found for the given topic\"\n",
    "                }\n",
    "            \n",
    "            summaries = self.summarizer.batch_summarize(unique_papers, summary_type)\n",
    "            \n",
    "            saved_files = []\n",
    "            print(f\"💾 Saving {len(unique_papers)} papers and summaries...\")\n",
    "            \n",
    "            for i, paper in enumerate(unique_papers):\n",
    "                paper_file = self.organizer.save_paper_data(paper, topic_folder)\n",
    "                if paper_file:\n",
    "                    saved_files.append(paper_file)\n",
    "                \n",
    "                summary_data = summaries.get(paper.title, {})\n",
    "                if summary_data.get('summary'):\n",
    "                    summary_file = self.organizer.save_summary(paper.title, summary_data, topic_folder)\n",
    "                    if summary_file:\n",
    "                        saved_files.append(summary_file)\n",
    "                \n",
    "                print(f\"   ✅ Saved paper {i+1}/{len(unique_papers)}\")\n",
    "            \n",
    "            report_file = self.organizer.create_research_report(topic, unique_papers, summaries, topic_folder)\n",
    "            if report_file:\n",
    "                saved_files.append(report_file)\n",
    "            \n",
    "            result = {\n",
    "                \"topic\": topic,\n",
    "                \"papers_found\": len(unique_papers),\n",
    "                \"folder_path\": str(topic_folder),\n",
    "                \"files_created\": saved_files,\n",
    "                \"papers\": unique_papers,\n",
    "                \"summaries\": summaries,\n",
    "                \"report_file\": report_file,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n🎉 Research completed successfully!\")\n",
    "            print(f\"📁 Files saved to: {topic_folder}\")\n",
    "            print(f\"📄 Created {len(saved_files)} files\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"❌ Error during research: {error_msg}\")\n",
    "            return {\n",
    "                \"topic\": topic,\n",
    "                \"papers_found\": 0,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": error_msg\n",
    "            }\n",
    "    \n",
    "    def _remove_duplicate_papers(self, papers: List[Paper]) -> List[Paper]:\n",
    "        unique_papers = []\n",
    "        seen_titles = set()\n",
    "        \n",
    "        for paper in papers:\n",
    "            title_key = re.sub(r'[^\\w\\s]', '', paper.title.lower())[:60]\n",
    "            if title_key not in seen_titles:\n",
    "                seen_titles.add(title_key)\n",
    "                unique_papers.append(paper)\n",
    "        \n",
    "        return unique_papers\n",
    "\n",
    "def create_kaggle_gradio_interface(assistant: PersonalResearchAssistant):\n",
    "    def research_interface(topic, max_papers, sources, summary_type, api_key):\n",
    "        if api_key and api_key.strip():\n",
    "            assistant.summarizer.api_key = api_key.strip()\n",
    "            assistant.summarizer.mock_mode = False\n",
    "            try:\n",
    "                import openai\n",
    "                openai.api_key = api_key.strip()\n",
    "                assistant.summarizer.openai = openai\n",
    "                print(\"🔑 API key updated!\")\n",
    "            except ImportError:\n",
    "                print(\"⚠️ OpenAI package not available\")\n",
    "        \n",
    "        source_list = []\n",
    "        if isinstance(sources, list):\n",
    "            if \"ArXiv\" in sources:\n",
    "                source_list.append(\"arxiv\")\n",
    "            if \"Semantic Scholar\" in sources:\n",
    "                source_list.append(\"semantic_scholar\")\n",
    "        \n",
    "        if not source_list:\n",
    "            return \"❌ Please select at least one source.\", \"\", \"\"\n",
    "        \n",
    "        if not topic.strip():\n",
    "            return \"❌ Please enter a research topic.\", \"\", \"\"\n",
    "        \n",
    "        if len(topic.strip()) < 3:\n",
    "            return \"❌ Please enter a more specific research topic (at least 3 characters).\", \"\", \"\"\n",
    "        \n",
    "        try:\n",
    "            result = assistant.research_topic(\n",
    "                topic=topic.strip(),\n",
    "                max_papers=max_papers,\n",
    "                sources=source_list,\n",
    "                summary_type=summary_type\n",
    "            )\n",
    "            \n",
    "            if result[\"status\"] == \"error\":\n",
    "                return f\"❌ Error: {result['error']}\", \"\", \"\"\n",
    "            \n",
    "            summary_text = f\"\"\"✅ **Research Completed Successfully!**\n",
    "\n",
    "**Topic:** {result['topic']}\n",
    "**Papers Found:** {result['papers_found']}\n",
    "**Files Created:** {len(result['files_created'])}\n",
    "**Folder Path:** {result['folder_path']}\n",
    "\n",
    "**Summary Statistics:**\n",
    "- Total papers processed: {result['papers_found']}\n",
    "- Files generated: {len(result['files_created'])}\n",
    "- Research report: Generated ✅\n",
    "\n",
    "**Created Files:**\"\"\"\n",
    "            \n",
    "            for file_path in result['files_created'][:10]:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                summary_text += f\"\\n• {file_name}\"\n",
    "            \n",
    "            if len(result['files_created']) > 10:\n",
    "                summary_text += f\"\\n• ... and {len(result['files_created']) - 10} more files\"\n",
    "            \n",
    "            papers_overview = \"# 📚 Papers Found\\n\\n\"\n",
    "            \n",
    "            for i, paper in enumerate(result['papers'][:8], 1):\n",
    "                papers_overview += f\"## {i}. {paper.title}\\n\\n\"\n",
    "                \n",
    "                authors_text = ', '.join(paper.authors[:4])\n",
    "                if len(paper.authors) > 4:\n",
    "                    authors_text += f\" *and {len(paper.authors)-4} others*\"\n",
    "                papers_overview += f\"**Authors:** {authors_text}\\n\\n\"\n",
    "                \n",
    "                metadata = []\n",
    "                if paper.year:\n",
    "                    metadata.append(f\"Year: {paper.year}\")\n",
    "                if paper.venue:\n",
    "                    metadata.append(f\"Venue: {paper.venue}\")\n",
    "                if paper.citations:\n",
    "                    metadata.append(f\"Citations: {paper.citations}\")\n",
    "                \n",
    "                if metadata:\n",
    "                    papers_overview += f\"**Details:** {' | '.join(metadata)}\\n\\n\"\n",
    "                \n",
    "                abstract_text = paper.abstract[:300]\n",
    "                if len(paper.abstract) > 300:\n",
    "                    abstract_text += \"...\"\n",
    "                papers_overview += f\"**Abstract:** {abstract_text}\\n\\n\"\n",
    "                \n",
    "                summary_data = result['summaries'].get(paper.title, {})\n",
    "                if summary_data.get('summary'):\n",
    "                    summary_text_short = summary_data['summary'][:200]\n",
    "                    if len(summary_data['summary']) > 200:\n",
    "                        summary_text_short += \"...\"\n",
    "                    papers_overview += f\"**AI Summary:** {summary_text_short}\\n\\n\"\n",
    "                \n",
    "                papers_overview += \"---\\n\\n\"\n",
    "            \n",
    "            if len(result['papers']) > 8:\n",
    "                papers_overview += f\"*... and {len(result['papers']) - 8} more papers in the full report*\\n\\n\"\n",
    "            \n",
    "            report_content = \"\"\n",
    "            if result.get('report_file') and os.path.exists(result['report_file']):\n",
    "                try:\n",
    "                    with open(result['report_file'], 'r', encoding='utf-8') as f:\n",
    "                        report_content = f.read()\n",
    "                        \n",
    "                    if len(report_content) > 10000:\n",
    "                        report_content = report_content[:10000] + \"\\n\\n... [Report truncated for display. Full report saved to file] ...\"\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    report_content = f\"❌ Error reading report file: {e}\"\n",
    "            else:\n",
    "                report_content = \"❌ Report file not found\"\n",
    "            \n",
    "            return summary_text, papers_overview, report_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Unexpected error during research: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg, \"\", \"\"\n",
    "    \n",
    "    interface = gr.Interface(\n",
    "        fn=research_interface,\n",
    "        inputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"🔍 Research Topic\",\n",
    "                placeholder=\"Enter your research topic (e.g., 'machine learning interpretability', 'quantum computing', 'natural language processing')\",\n",
    "                lines=2,\n",
    "                max_lines=3\n",
    "            ),\n",
    "            gr.Slider(\n",
    "                minimum=5,\n",
    "                maximum=25,\n",
    "                value=10,\n",
    "                step=5,\n",
    "                label=\"📊 Maximum Papers\",\n",
    "                info=\"Number of papers to fetch (5-25)\"\n",
    "            ),\n",
    "            gr.CheckboxGroup(\n",
    "                choices=[\"ArXiv\", \"Semantic Scholar\"],\n",
    "                value=[\"ArXiv\", \"Semantic Scholar\"],\n",
    "                label=\"📚 Paper Sources\",\n",
    "                info=\"Select one or more sources\"\n",
    "            ),\n",
    "            gr.Radio(\n",
    "                choices=[\"brief\", \"comprehensive\", \"technical\"],\n",
    "                value=\"comprehensive\",\n",
    "                label=\"📝 Summary Type\",\n",
    "                info=\"Choose the type of AI-generated summary\"\n",
    "            ),\n",
    "            gr.Textbox(\n",
    "                label=\"🔑 OpenAI API Key (Optional)\",\n",
    "                placeholder=\"Enter your OpenAI API key for AI summaries (or leave blank for structured summaries)\",\n",
    "                type=\"password\",\n",
    "                info=\"Required only for OpenAI-powered summaries\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"📊 Research Summary\", \n",
    "                lines=12,\n",
    "                max_lines=15\n",
    "            ),\n",
    "            gr.Markdown(\n",
    "                label=\"📚 Papers Overview\",\n",
    "                height=400\n",
    "            ),\n",
    "            gr.Textbox(\n",
    "                label=\"📄 Full Research Report\", \n",
    "                lines=25,\n",
    "                max_lines=30\n",
    "            )\n",
    "        ],\n",
    "        title=\"🔬 Personal Research Assistant\",\n",
    "        description=\"\"\"\n",
    "        ### Your AI-Powered Research Companion 🤖\n",
    "        \n",
    "        This tool helps you discover, organize, and summarize academic papers from ArXiv and Semantic Scholar.\n",
    "        \n",
    "        **Features:**\n",
    "        - 📚 Search multiple academic databases\n",
    "        - 🤖 AI-powered paper summaries (with OpenAI API key)\n",
    "        - 📁 Organized file system with structured notes\n",
    "        - 📊 Comprehensive research reports\n",
    "        \n",
    "        **Usage:**\n",
    "        1. Enter your research topic\n",
    "        2. Select paper sources and summary type\n",
    "        3. Optionally provide OpenAI API key for enhanced summaries\n",
    "        4. Click Submit to start research\n",
    "        \n",
    "        Files are saved to `/kaggle/working/research_notes/`\n",
    "        \"\"\",\n",
    "        examples=[\n",
    "            [\"machine learning interpretability\", 10, [\"ArXiv\", \"Semantic Scholar\"], \"comprehensive\", \"\"],\n",
    "            [\"quantum computing algorithms\", 15, [\"ArXiv\"], \"technical\", \"\"],\n",
    "            [\"transformer neural networks\", 12, [\"Semantic Scholar\"], \"brief\", \"\"],\n",
    "            [\"reinforcement learning robotics\", 8, [\"ArXiv\", \"Semantic Scholar\"], \"comprehensive\", \"\"]\n",
    "        ],\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        .gradio-container {\n",
    "            max-width: 1200px !important;\n",
    "        }\n",
    "        .description {\n",
    "            font-size: 14px;\n",
    "        }\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "def run_demo_research():\n",
    "    print(\"🧪 Running demo research...\")\n",
    "    \n",
    "    assistant = PersonalResearchAssistant()\n",
    "    \n",
    "    result = assistant.research_topic(\n",
    "        topic=\"machine learning\",\n",
    "        max_papers=5,\n",
    "        sources=[\"arxiv\"],\n",
    "        summary_type=\"brief\"\n",
    "    )\n",
    "    \n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(f\"✅ Demo completed! Found {result['papers_found']} papers\")\n",
    "        print(f\"📁 Files saved to: {result['folder_path']}\")\n",
    "        \n",
    "        print(\"\\n📄 Created files:\")\n",
    "        for file_path in result['files_created']:\n",
    "            print(f\"   • {os.path.basename(file_path)}\")\n",
    "    else:\n",
    "        print(f\"❌ Demo failed: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    print(\"🔬 Personal Research Assistant - Kaggle Edition\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    assistant = PersonalResearchAssistant()\n",
    "    \n",
    "    interface = create_kaggle_gradio_interface(assistant)\n",
    "    \n",
    "    interface.launch(\n",
    "        share=True,\n",
    "        debug=False,\n",
    "        show_error=True,\n",
    "        height=800,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
